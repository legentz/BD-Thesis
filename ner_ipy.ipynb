{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import keras as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, dev_test=False):\n",
    "        self.dev_test = dev_test\n",
    "\n",
    "        #Hyperparams\n",
    "        self.context_length = 10\n",
    "        self.batch_size = 1000\n",
    "        self.step_per_epoch = 2000\n",
    "        self.nb_epochs = 5\n",
    "        self.emb_dim = 300\n",
    "        self.target_dim = 113\n",
    "        self.dropout_ = 0.5\n",
    "        self.learning_rate = 0.001\n",
    "        self.feature_size = 600000\n",
    "        self.lstm_dim = 100\n",
    "        self.attention_dim = 100 # dim of attention module\n",
    "        self.feature_dim = 50 # dim of feature representation\n",
    "        self.feature_input_dim = 70\n",
    "        self.rep_dim = self.lstm_dim * 2 + self.emb_dim # if encoder is not 'averanging'\n",
    "        self.rep_dim += self.feature_dim # if --feature\n",
    "        self.model_metrics = ['accuracy']\n",
    "\n",
    "        self.model = K.models.Sequential()\n",
    "\n",
    "        # Placeholders with Tensorflow\n",
    "        # self.keep_prob = K.backend.placeholder(dtype='float32') # K.backend.placeholder((2, 3), dtype='float32')\n",
    "        # self.mention_representation = K.backend.placeholder((None, self.emb_dim), dtype='float32')\n",
    "        # self.context = [K.backend.placeholder((None, self.emb_dim), dtype='float32') for _ in range((self.context_length * 2) + 1)]\n",
    "        # self.target = K.backend.placeholder((None, self.target_dim), dtype='float32')\n",
    "\n",
    "        # Trying to use ---- tensor\n",
    "        self.mention_representation = K.layers.Input(shape=(self.emb_dim,))\n",
    "        self.context = K.layers.Input(batch_shape=(self.batch_size, self.context_length*2+1, self.emb_dim))\n",
    "        self.target = K.layers.Input(shape=(self.target_dim,))\n",
    "\n",
    "        # Dropout and split context into L/R\n",
    "        # Dropout with Keras has a problem... so we have to use tf.nn.dropout!\n",
    "        # mention_representation_dropout = tf.nn.dropout(mention_representation, keep_prob)\n",
    "        self.mention_representation_dropout = K.backend.dropout(self.mention_representation, self.dropout_)\n",
    "        self.left_context = self.context[:self.context_length]\n",
    "        self.right_context = self.context[self.context_length + 1:]\n",
    "\n",
    "        print 'Context placeholder created!'\n",
    "\n",
    "        # if --attentive (LSTM + Attentions)\n",
    "        self.left_oneLSTM = K.layers.recurrent.LSTM(self.lstm_dim, stateful=True)(self.left_context) # stateful=True,\n",
    "        self.right_oneLSTM = K.layers.recurrent.LSTM(self.lstm_dim, stateful=True, go_backwards=True)(self.right_context) # stateful=True,\n",
    "        self.left_biLSTM = K.layers.wrappers.Bidirectional(self.left_oneLSTM, merge_mode='concat') # (self.left_context)\n",
    "        self.right_biLSTM = K.layers.wrappers.Bidirectional(self.right_oneLSTM, merge_mode='concat') # (self.right_context)\n",
    "\n",
    "        print 'biLSTM created!'\n",
    "\n",
    "        # Updating model\n",
    "        self.model.add(self.left_biLSTM)\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(self.right_biLSTM)\n",
    "        self.model.add(Dropout(0.5))\n",
    "\n",
    "        print 'biLSTM added to model!'\n",
    "\n",
    "        # Dev\n",
    "        if self.dev_test:\n",
    "            print 'Got murdered :('\n",
    "            sys.exit(1)\n",
    "\n",
    "        self.merge_biLSTM = merge([self.left_biLSTM, self.right_biLSTM], mode='sum')\n",
    "\n",
    "        self.attention = Dense(self.attention_dim, activation='tanh', input_shape=self.lstm_dim*2)(self.merge_biLSTM)\n",
    "        self.attention = Flatten()(self.attention)\n",
    "        self.attention = Activation('softmax')(self.attention)\n",
    "        self.attention = RepeatVector(self.lstm_dim)(self.attention)\n",
    "        self.attention = Permute([2, 1])(self.attention)\n",
    "\n",
    "        self.context_representation = merge([self.activations, self.attention], mode='mul')\n",
    "\n",
    "        # Missing --feature part...\n",
    "        # ...\n",
    "        self.representation = K.layers.merge.Concatenate([self.mention_representation_dropout, self.context_representation], axis=1)\n",
    "\n",
    "        # Missing --hier part...\n",
    "        # ...\n",
    "        self.W = self.create_weight_variable('hier_W', (self.rep_dim, self.target_dim))\n",
    "        self.logit = K.layers.merge.Dot(self.representation, self.W)\n",
    "\n",
    "        self.distribution = K.sigmoid(self.logit)\n",
    "\n",
    "        self.loss_f = np.mean(K.backend.binary_crossentropy(self.logit, self.target, from_logits=True))\n",
    "        self.optimizer_adam = K.optimizers.Adam(lr=self.learning_rate)\n",
    "\n",
    "    def set_attention_layer(self, model):\n",
    "        # Set Attentions...\n",
    "        return True\n",
    "\n",
    "    def compile_model(self):\n",
    "        if self.model is not Null:\n",
    "            self.model.compile(optimizer=self.optimizer_adam, metrics=self.model_metrics, loss=self.loss_f, batch_size=self.batch_size)\n",
    "\n",
    "    def get_model_summary(self):\n",
    "        if self.model is not Null:\n",
    "            return self.model.summary()\n",
    "\n",
    "    def get_model(self):\n",
    "        if self.model is not Null:\n",
    "            return self.model\n",
    "\n",
    "    def create_weight_variable(self, name, shape, pad=True):\n",
    "        initial = np.random.uniform(-0.01, 0.01, size=shape)\n",
    "        # initial = K.random_uniform_variable(shape=shape, -0.01, 0.01)\n",
    "\n",
    "        if pad == True:\n",
    "            initial[0] = np.zeros(shape[1])\n",
    "\n",
    "        # initial = tf.constant_initializer(initial)\n",
    "        initial = tf.contrib.keras.initializers.Constant(initial)\n",
    "\n",
    "        return K.backend.variable(value=initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context placeholder created!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'get_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-9a64aa89c35b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-0a22e04eb886>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dev_test)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_oneLSTM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_context\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# stateful=True,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_oneLSTM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgo_backwards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_context\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# stateful=True,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_biLSTM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_oneLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'concat'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (self.left_context)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_biLSTM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_oneLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'concat'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (self.right_context)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/layers/wrappers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layer, merge_mode, weights, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                              '{\"sum\", \"mul\", \"ave\", \"concat\", None}')\n\u001b[1;32m    254\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'go_backwards'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'go_backwards'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'get_config'"
     ]
    }
   ],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
